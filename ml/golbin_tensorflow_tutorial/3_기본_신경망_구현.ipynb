{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망\n",
    "입력값 X에 가중치(W)를 곱하고 편향(b)을 더한 뒤 활성화 함수(Sigmoid, ReLU 등)을 거쳐 결과값 y를 만들어내는 것이 인공 신경망의 기본이다.  \n",
    "원하는 y값을 만들어내기 위해 W와 b의 값을 변경해가면서 적절한 값을 찾아내는 최적화 과정을 학습(learning) 또는 훈련(training)  \n",
    "\n",
    "활성화 함수(activation function)는 인공신경망을 통과한 값을 최종적으로 어떤 값으로 만들지 결정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 분류 모델 구현\n",
    "패턴을 파악해 여러 종류로 구분하는 작업을 **분류(classification)** 이라고 한다.\n",
    "\n",
    "털과 날개가 있느냐를 기준으로 포유류와 조류를 구분하는 신경망 모델을 만들어본다.  \n",
    "\n",
    "먼저 텐서플로와 행렬조작과 연산에 필수라 할 수 있는 NumPY 라이브러리를 임포트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용할 데이터를 정의하고, 각 개체가 실제 어떤 종류인지 나타내는 레이블(분류값) 데이터를 원-핫 인코딩(one-hot encoding) 형태로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X`와 `Y`에 실측값(ground truth)을 넣어서 학습시킬 것이기 때문에 `X`와 `Y`는 플레이스홀더로 설정한다.  \n",
    "그 다음 신경망을 결정하는 가중치(`W`)와 편향 변수(`b`)를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치(`W`)는 `[입력층(특징 수), 출력층(레이블 수)]`의 구성이고, 편향 변수(`b`)는 레이블 수인 3개의 요소를 가진 변수로 설정한다.  \n",
    "이 가중치를 곱하고 편향을 더한 결과를 활성화 함수인 **ReLU**에 적용하면 신경망 구성은 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.add(tf.matmul(X, W), b)\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 통해 나온 출력값을 `softmax`함수를 이용해서 사용하기 쉽게 다듬어준다.  \n",
    "`softmax`함수는 배열 내의 결과값들을 전체 합이 1이 되도록 만들어준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 3) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 손실함수를 작성한다.  \n",
    "원-핫 인코딩 형식을 이용하는 대부분의 모델에서 사용하는 **교차 엔트로피(Cross-Entropy)** 함수를 사용한다.  \n",
    "(교차 엔트로피 값은 예측값과 실제값 사이의 확률 분포 차이를 계산한 값)\n",
    "\n",
    "> Y는 실측값이고, model은 신경망을 통해 나온 예측값이다.   \n",
    "> model 값에 log를 취한 후 Y와 곱하고..  \n",
    "> 행별로 값을 다 더한 후 배열 안 값의 평균을 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 시켜본다.  \n",
    "기본적인 경상하강법으로 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.0939702\n",
      "20 1.0927329\n",
      "30 1.0915931\n",
      "40 1.0903736\n",
      "50 1.0892487\n",
      "60 1.0880402\n",
      "70 1.0869329\n",
      "80 1.0857421\n",
      "90 1.0846379\n",
      "100 1.0835295\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측값인 model을 바로 출력하면 `[0.2 0.7 0.1]` 같이 확률로 나오기 때문에..  \n",
    "이 요소 중 가장 큰 값의 인덱스를 찾아주는 `argmax` 함수를 사용해서 레이블 값을 출력하게 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 0 0 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(model, axis=1)\n",
    "target = tf.argmax(Y, axis=1)\n",
    "\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 학습 데이터에 대한 예측값과 실제값을 `tf.equal` 함수로 비교한 뒤 `true`, `false`로 나온 결과를 다시 `tf.cast` 함수를 이용해서 0과 1로 바꾸어 평균을 내면 간단히 정확도를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층 신경망 구현하기\n",
    "다층 신경망을 만드는 것은 매우 간단하다.  \n",
    "앞서 만든 신경망 모델에 가중치와 편향을 추가하기만 하면 된다.  \n",
    "\n",
    "입력층과 출력층은 각각 특징과 분류의 개수로 맞추고, 중간의 연결 부분(은닉층, hidden layer)은 맞닿은 층의 뉴런 수와 같도록 맞춘다.  \n",
    "\n",
    "```\n",
    "W1 = [2, 10]  -> [특징, 은닉층의 뉴런 수]\n",
    "W2 = [10, 3]  -> [은닉층의 뉴런 수, 분류 수]\n",
    "\n",
    "b1 = [10]     -> 은닉층의 뉴런 수\n",
    "b2 = [3]      -> 분류 수\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 입력값에 첫번째 가중치와 편향, 그리고 활성화 함수를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층을 만들기 위해 두번째 가중치와 편향을 적용하여 최종 모델을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.add(tf.matmul(L1, W2), b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수와 최적화 함수를 통해 학습을 시작한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `AdamOptimizer` 최적화 함수를 사용한다. 전에 사용한 `GradientDecentOptimizer` 보다 보편적으로 성능이 좋다고 알려져있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.94237685\n",
      "20 0.7045836\n",
      "30 0.5452649\n",
      "40 0.4343115\n",
      "50 0.35064474\n",
      "60 0.28560346\n",
      "70 0.22535618\n",
      "80 0.16653451\n",
      "90 0.12177116\n",
      "100 0.088799536\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 100.00\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도 %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝의 세계에 온 걸 환영한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
