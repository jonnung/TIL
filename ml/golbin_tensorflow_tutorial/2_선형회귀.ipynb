{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로 - 선형회귀 모델\n",
    "\n",
    "**선형 회귀** 란? 주어진 x와 y값을 가지고 서로 간의 관계를 파악하는 것.\n",
    "이 관계를 알고 나면 새로운 x값이 주어졌을 때 y값을 쉽게 알 수 있다.  \n",
    "\n",
    "아래 `x_data`와 `y_data`의 상관관계를 파악해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x와 y의 상관관계를 설명하기 위한 변수들인 W와 b를 각각 -1.0부터 1.0사이의 <u>균등분포(uniform distribution)</u>를 가진 무작위 값으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자료를 입력 받을 플레이스홀더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X와 Y의 상관관계(선형관계)를 분석하기 위한 수식\n",
    "\n",
    "`W`는 가중치(weight)  \n",
    "`b`는 편향(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수(loss fuction)는 한 쌍(x, y)의 데이터에 대한 **손실값**을 계산하는 함수.  \n",
    "손실값이란 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값.  \n",
    "즉, 손실값이 작을수록 그 모델이 X와 Y의 관계를 잘 설명하고 있다는 뜻이며, 주어진 X값에 대한 Y값을 정확하게 예측할 수 있다는 뜻.  \n",
    "이 손실을 전체 데이터에 대해 구한 경우 이를 **비용(cost)** 라고 한다.  \n",
    "**학습**이란 변수들의 값을 다양하게 넣어 계산해보면서 이 손실값을 최소화 하는 `W`와 `b` 값을 구하는 것.  \n",
    "손실값은 예측값에서 실제값을 뺀 뒤 제곱하여, 그리고 비용은 모든 데이터에 대한 손실값의 평균을 내어 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최적화 함수**란 가중치(W)와 편향(b) 값을 변경해가면서 손실값을 최소화하는 가장 최적화된 가중치와 편향값을 찾아주는 함수  \n",
    "이 값들을 무작위로 변경하면 시간이 너무 오래 걸리고, 학습 시간 예측이 어렵기 때문에 빠르게 최적화하기 위한 다양한 방법을 활용한다.  \n",
    "**경상하강법(gradient descent)** 은 함수의 기울기를 구하고 기울기가 낮은 쪽으로 계속 이동시키면서 최적화의 값을 찾아 나가는 방법.  \n",
    "최적화 함수의 매개변수인 **학습률(learning_rate)** 은 학습을 얼마나 '급하게' 할 것인가를 설정하는 값으로 값이 너무 크면 최적의 손실값을 찾지 못하고 지나치게 되고, 값이 너무 작으면 학습 속도가 너무 느려진다. 이렇게 학습을 진행하는 과정에 영향을 주는 변수를 **하이퍼파라미터(hyperparameter)** 라고 한다. 머신러닝에서는 이 하이퍼파라미터를 잘 튜닝하는 것이 큰 과제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.488463 [0.88190377] [0.4080097]\n",
      "1 0.03881898 [0.828923] [0.37364626]\n",
      "2 0.020503325 [0.83913636] [0.3673478]\n",
      "3 0.019332647 [0.84233665] [0.3582237]\n",
      "4 0.018411972 [0.84619963] [0.3496443]\n",
      "5 0.017537368 [0.8498889] [0.34123558]\n",
      "6 0.016704306 [0.85349834] [0.33303288]\n",
      "7 0.015910862 [0.8570201] [0.325027]\n",
      "8 0.015155096 [0.8604572] [0.31721357]\n",
      "9 0.014435202 [0.86381173] [0.309588]\n",
      "10 0.01374952 [0.8670856] [0.3021457]\n",
      "11 0.013096403 [0.87028074] [0.2948823]\n",
      "12 0.012474318 [0.87339914] [0.28779355]\n",
      "13 0.011881781 [0.8764425] [0.28087518]\n",
      "14 0.011317376 [0.8794128] [0.27412313]\n",
      "15 0.010779787 [0.8823116] [0.2675334]\n",
      "16 0.01026776 [0.8851408] [0.26110208]\n",
      "17 0.009780024 [0.8879019] [0.25482535]\n",
      "18 0.009315462 [0.8905966] [0.24869952]\n",
      "19 0.008872982 [0.8932267] [0.24272098]\n",
      "20 0.008451498 [0.8957934] [0.2368861]\n",
      "21 0.008050057 [0.8982985] [0.23119153]\n",
      "22 0.0076676607 [0.90074325] [0.22563381]\n",
      "23 0.007303449 [0.90312934] [0.22020975]\n",
      "24 0.006956522 [0.90545803] [0.21491605]\n",
      "25 0.006626088 [0.90773076] [0.20974962]\n",
      "26 0.0063113435 [0.9099489] [0.2047074]\n",
      "27 0.006011553 [0.9121136] [0.19978635]\n",
      "28 0.0057259966 [0.91422635] [0.19498363]\n",
      "29 0.0054540033 [0.91628826] [0.19029635]\n",
      "30 0.005194938 [0.9183007] [0.18572178]\n",
      "31 0.004948171 [0.92026466] [0.18125714]\n",
      "32 0.0047131344 [0.9221814] [0.17689985]\n",
      "33 0.004489251 [0.9240521] [0.17264731]\n",
      "34 0.004276023 [0.9258779] [0.16849701]\n",
      "35 0.0040728995 [0.92765975] [0.16444644]\n",
      "36 0.0038794372 [0.9293988] [0.16049327]\n",
      "37 0.0036951546 [0.9310959] [0.15663509]\n",
      "38 0.0035196366 [0.9327524] [0.1528697]\n",
      "39 0.0033524493 [0.93436897] [0.1491948]\n",
      "40 0.003193212 [0.9359467] [0.14560828]\n",
      "41 0.0030415289 [0.93748647] [0.14210795]\n",
      "42 0.0028970453 [0.9389893] [0.13869178]\n",
      "43 0.0027594306 [0.94045585] [0.13535771]\n",
      "44 0.0026283674 [0.9418873] [0.13210383]\n",
      "45 0.0025035152 [0.9432843] [0.12892814]\n",
      "46 0.0023845977 [0.94464767] [0.12582879]\n",
      "47 0.0022713281 [0.94597834] [0.12280398]\n",
      "48 0.0021634407 [0.94727695] [0.11985185]\n",
      "49 0.0020606748 [0.94854444] [0.11697071]\n",
      "50 0.0019627882 [0.94978136] [0.11415879]\n",
      "51 0.0018695543 [0.9509886] [0.11141449]\n",
      "52 0.0017807496 [0.9521668] [0.10873615]\n",
      "53 0.0016961638 [0.9533167] [0.10612222]\n",
      "54 0.0016155938 [0.95443887] [0.10357109]\n",
      "55 0.0015388513 [0.95553416] [0.10108133]\n",
      "56 0.0014657537 [0.9566031] [0.0986514]\n",
      "57 0.0013961258 [0.9576463] [0.09627988]\n",
      "58 0.0013298165 [0.9586645] [0.09396538]\n",
      "59 0.0012666454 [0.95965815] [0.09170651]\n",
      "60 0.001206479 [0.9606279] [0.08950195]\n",
      "61 0.0011491749 [0.9615745] [0.08735042]\n",
      "62 0.0010945867 [0.9624981] [0.08525054]\n",
      "63 0.0010425904 [0.96339965] [0.08320118]\n",
      "64 0.0009930665 [0.96427953] [0.08120109]\n",
      "65 0.000945897 [0.96513826] [0.07924907]\n",
      "66 0.0009009621 [0.96597624] [0.07734395]\n",
      "67 0.00085816765 [0.9667942] [0.07548466]\n",
      "68 0.0008174041 [0.9675924] [0.07367005]\n",
      "69 0.000778577 [0.96837145] [0.07189907]\n",
      "70 0.0007415924 [0.96913177] [0.07017066]\n",
      "71 0.00070636853 [0.96987385] [0.06848381]\n",
      "72 0.00067281345 [0.9705981] [0.06683753]\n",
      "73 0.00064085727 [0.9713049] [0.06523079]\n",
      "74 0.00061041495 [0.97199464] [0.06366266]\n",
      "75 0.00058141776 [0.9726679] [0.06213226]\n",
      "76 0.0005538015 [0.97332495] [0.06063865]\n",
      "77 0.0005274942 [0.97396624] [0.05918096]\n",
      "78 0.0005024375 [0.97459203] [0.05775826]\n",
      "79 0.00047857084 [0.9752028] [0.05636979]\n",
      "80 0.0004558399 [0.97579896] [0.05501471]\n",
      "81 0.0004341896 [0.97638077] [0.0536922]\n",
      "82 0.00041356395 [0.97694856] [0.05240147]\n",
      "83 0.00039391674 [0.97750264] [0.05114174]\n",
      "84 0.0003752076 [0.97804344] [0.04991232]\n",
      "85 0.0003573863 [0.97857136] [0.0487125]\n",
      "86 0.00034041013 [0.9790864] [0.04754145]\n",
      "87 0.00032423975 [0.9795892] [0.04639861]\n",
      "88 0.00030883707 [0.9800798] [0.04528319]\n",
      "89 0.00029416752 [0.98055875] [0.04419464]\n",
      "90 0.0002801934 [0.98102605] [0.04313221]\n",
      "91 0.0002668844 [0.9814822] [0.04209534]\n",
      "92 0.0002542069 [0.98192734] [0.04108339]\n",
      "93 0.00024213322 [0.9823618] [0.04009578]\n",
      "94 0.00023063076 [0.9827858] [0.0391319]\n",
      "95 0.00021967408 [0.9831996] [0.03819119]\n",
      "96 0.00020924135 [0.98360354] [0.03727312]\n",
      "97 0.00019930024 [0.98399764] [0.03637708]\n",
      "98 0.00018983388 [0.98438233] [0.0355026]\n",
      "99 0.00018081717 [0.9847578] [0.03464914]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, cost_val, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용되지 않았던 값인 5와 2.5를 X값으로 넣고 결과를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9584384], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(hypothesis, feed_dict={X: 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4965436], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(hypothesis, feed_dict={X: 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
